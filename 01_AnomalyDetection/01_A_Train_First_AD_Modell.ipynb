{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Detection Model - FIRST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modellbeschreibung:\n",
    "\n",
    "- Training auf den Jahren 2020 und 2021\n",
    "- Adam als Opzimizer\n",
    "- Komplexerer AE\n",
    "- Mehr als 6 Epochen\n",
    "- Normalisierung der Daten\n",
    "- Modell wie aus der Vorstudie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ./scripts/PythonImports.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Configs and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of normal data: (105216, 17)\n",
      "Shape of anormal data: (35040, 18)\n"
     ]
    }
   ],
   "source": [
    "%run -i ./scripts/TrainPreperations.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skalieren der Daten & \"Column name zu Zahl\"-Mapping speichern & Daten f√ºr PyTorch vorbereiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ./scripts/ScaleAndPrepare.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x130e45990>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.SimpleAutoEncoder import SimpleAutoEncoder\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Inputs: 17\n",
      "SimpleAutoEncoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=17, out_features=12, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=12, out_features=8, bias=True)\n",
      "    (3): Tanh()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=8, out_features=12, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=12, out_features=17, bias=True)\n",
      "    (3): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "num_inputs = len(df_data.columns)\n",
    "print('Num Inputs: {}'.format(num_inputs))\n",
    "\n",
    "val_lambda = 0.5\n",
    "model = SimpleAutoEncoder(num_inputs=num_inputs, val_lambda=val_lambda)\n",
    "critereon = mse_loss\n",
    "optimizer = Adam(model.parameters())\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Epoch: 1/32 [128/105216 (0%)]\tLoss: 0.318232\n",
      " Train Epoch: 1/32 [6528/105216 (6%)]\tLoss: 0.302905\n",
      " Train Epoch: 1/32 [12928/105216 (12%)]\tLoss: 0.162769\n",
      " Train Epoch: 1/32 [19328/105216 (18%)]\tLoss: 0.090886\n",
      " Train Epoch: 1/32 [25728/105216 (24%)]\tLoss: 0.052184\n",
      " Train Epoch: 1/32 [32128/105216 (31%)]\tLoss: 0.052721\n",
      " Train Epoch: 1/32 [38528/105216 (37%)]\tLoss: 0.049132\n",
      " Train Epoch: 1/32 [44928/105216 (43%)]\tLoss: 0.046855\n",
      " Train Epoch: 1/32 [51328/105216 (49%)]\tLoss: 0.045942\n",
      " Train Epoch: 1/32 [57728/105216 (55%)]\tLoss: 0.029837\n",
      " Train Epoch: 1/32 [64128/105216 (61%)]\tLoss: 0.024614\n",
      " Train Epoch: 1/32 [70528/105216 (67%)]\tLoss: 0.030230\n",
      " Train Epoch: 1/32 [76928/105216 (73%)]\tLoss: 0.048548\n",
      " Train Epoch: 1/32 [83328/105216 (79%)]\tLoss: 0.011161\n",
      " Train Epoch: 1/32 [89728/105216 (85%)]\tLoss: 0.034344\n",
      " Train Epoch: 1/32 [96128/105216 (91%)]\tLoss: 0.012327\n",
      " Train Epoch: 1/32 [102528/105216 (97%)]\tLoss: 0.019800\n",
      " Train Epoch: 2/32 [128/105216 (0%)]\tLoss: 0.018064\n",
      " Train Epoch: 2/32 [6528/105216 (6%)]\tLoss: 0.006677\n",
      " Train Epoch: 2/32 [12928/105216 (12%)]\tLoss: 0.009806\n",
      " Train Epoch: 2/32 [19328/105216 (18%)]\tLoss: 0.008649\n",
      " Train Epoch: 2/32 [25728/105216 (24%)]\tLoss: 0.005883\n",
      " Train Epoch: 2/32 [32128/105216 (31%)]\tLoss: 0.011627\n",
      " Train Epoch: 2/32 [38528/105216 (37%)]\tLoss: 0.011984\n",
      " Train Epoch: 2/32 [44928/105216 (43%)]\tLoss: 0.008942\n",
      " Train Epoch: 2/32 [51328/105216 (49%)]\tLoss: 0.007893\n",
      " Train Epoch: 2/32 [57728/105216 (55%)]\tLoss: 0.007085\n",
      " Train Epoch: 2/32 [64128/105216 (61%)]\tLoss: 0.008816\n",
      " Train Epoch: 2/32 [70528/105216 (67%)]\tLoss: 0.010014\n",
      " Train Epoch: 2/32 [76928/105216 (73%)]\tLoss: 0.016865\n",
      " Train Epoch: 2/32 [83328/105216 (79%)]\tLoss: 0.005350\n",
      " Train Epoch: 2/32 [89728/105216 (85%)]\tLoss: 0.011188\n",
      " Train Epoch: 2/32 [96128/105216 (91%)]\tLoss: 0.004213\n",
      " Train Epoch: 2/32 [102528/105216 (97%)]\tLoss: 0.008029\n",
      " Train Epoch: 3/32 [128/105216 (0%)]\tLoss: 0.008226\n",
      " Train Epoch: 3/32 [6528/105216 (6%)]\tLoss: 0.003993\n",
      " Train Epoch: 3/32 [12928/105216 (12%)]\tLoss: 0.005323\n",
      " Train Epoch: 3/32 [19328/105216 (18%)]\tLoss: 0.004715\n",
      " Train Epoch: 3/32 [25728/105216 (24%)]\tLoss: 0.003149\n",
      " Train Epoch: 3/32 [32128/105216 (31%)]\tLoss: 0.006274\n",
      " Train Epoch: 3/32 [38528/105216 (37%)]\tLoss: 0.009723\n",
      " Train Epoch: 3/32 [44928/105216 (43%)]\tLoss: 0.007354\n",
      " Train Epoch: 3/32 [51328/105216 (49%)]\tLoss: 0.004716\n",
      " Train Epoch: 3/32 [57728/105216 (55%)]\tLoss: 0.004195\n",
      " Train Epoch: 3/32 [64128/105216 (61%)]\tLoss: 0.007021\n",
      " Train Epoch: 3/32 [70528/105216 (67%)]\tLoss: 0.006492\n",
      " Train Epoch: 3/32 [76928/105216 (73%)]\tLoss: 0.010297\n",
      " Train Epoch: 3/32 [83328/105216 (79%)]\tLoss: 0.003877\n",
      " Train Epoch: 3/32 [89728/105216 (85%)]\tLoss: 0.007639\n",
      " Train Epoch: 3/32 [96128/105216 (91%)]\tLoss: 0.003104\n",
      " Train Epoch: 3/32 [102528/105216 (97%)]\tLoss: 0.006813\n",
      " Train Epoch: 4/32 [128/105216 (0%)]\tLoss: 0.006265\n",
      " Train Epoch: 4/32 [6528/105216 (6%)]\tLoss: 0.002862\n",
      " Train Epoch: 4/32 [12928/105216 (12%)]\tLoss: 0.003907\n",
      " Train Epoch: 4/32 [19328/105216 (18%)]\tLoss: 0.003113\n",
      " Train Epoch: 4/32 [25728/105216 (24%)]\tLoss: 0.002665\n",
      " Train Epoch: 4/32 [32128/105216 (31%)]\tLoss: 0.005464\n",
      " Train Epoch: 4/32 [38528/105216 (37%)]\tLoss: 0.007742\n",
      " Train Epoch: 4/32 [44928/105216 (43%)]\tLoss: 0.005646\n",
      " Train Epoch: 4/32 [51328/105216 (49%)]\tLoss: 0.003189\n",
      " Train Epoch: 4/32 [57728/105216 (55%)]\tLoss: 0.002750\n",
      " Train Epoch: 4/32 [64128/105216 (61%)]\tLoss: 0.004875\n",
      " Train Epoch: 4/32 [70528/105216 (67%)]\tLoss: 0.004802\n",
      " Train Epoch: 4/32 [76928/105216 (73%)]\tLoss: 0.006735\n",
      " Train Epoch: 4/32 [83328/105216 (79%)]\tLoss: 0.002533\n",
      " Train Epoch: 4/32 [89728/105216 (85%)]\tLoss: 0.004738\n",
      " Train Epoch: 4/32 [96128/105216 (91%)]\tLoss: 0.002461\n",
      " Train Epoch: 4/32 [102528/105216 (97%)]\tLoss: 0.004438\n",
      " Train Epoch: 5/32 [128/105216 (0%)]\tLoss: 0.004967\n",
      " Train Epoch: 5/32 [6528/105216 (6%)]\tLoss: 0.002001\n",
      " Train Epoch: 5/32 [12928/105216 (12%)]\tLoss: 0.002370\n",
      " Train Epoch: 5/32 [19328/105216 (18%)]\tLoss: 0.002051\n",
      " Train Epoch: 5/32 [25728/105216 (24%)]\tLoss: 0.001842\n",
      " Train Epoch: 5/32 [32128/105216 (31%)]\tLoss: 0.002828\n",
      " Train Epoch: 5/32 [38528/105216 (37%)]\tLoss: 0.003202\n",
      " Train Epoch: 5/32 [44928/105216 (43%)]\tLoss: 0.003312\n",
      " Train Epoch: 5/32 [51328/105216 (49%)]\tLoss: 0.001895\n",
      " Train Epoch: 5/32 [57728/105216 (55%)]\tLoss: 0.001638\n",
      " Train Epoch: 5/32 [64128/105216 (61%)]\tLoss: 0.002014\n",
      " Train Epoch: 5/32 [70528/105216 (67%)]\tLoss: 0.003002\n",
      " Train Epoch: 5/32 [76928/105216 (73%)]\tLoss: 0.002963\n",
      " Train Epoch: 5/32 [83328/105216 (79%)]\tLoss: 0.001350\n",
      " Train Epoch: 5/32 [89728/105216 (85%)]\tLoss: 0.002805\n",
      " Train Epoch: 5/32 [96128/105216 (91%)]\tLoss: 0.001467\n",
      " Train Epoch: 5/32 [102528/105216 (97%)]\tLoss: 0.002322\n",
      " Train Epoch: 6/32 [128/105216 (0%)]\tLoss: 0.003435\n",
      " Train Epoch: 6/32 [6528/105216 (6%)]\tLoss: 0.001272\n",
      " Train Epoch: 6/32 [12928/105216 (12%)]\tLoss: 0.001311\n",
      " Train Epoch: 6/32 [19328/105216 (18%)]\tLoss: 0.000972\n",
      " Train Epoch: 6/32 [25728/105216 (24%)]\tLoss: 0.001123\n",
      " Train Epoch: 6/32 [32128/105216 (31%)]\tLoss: 0.000865\n",
      " Train Epoch: 6/32 [38528/105216 (37%)]\tLoss: 0.001111\n",
      " Train Epoch: 6/32 [44928/105216 (43%)]\tLoss: 0.002014\n",
      " Train Epoch: 6/32 [51328/105216 (49%)]\tLoss: 0.001174\n",
      " Train Epoch: 6/32 [57728/105216 (55%)]\tLoss: 0.000894\n",
      " Train Epoch: 6/32 [64128/105216 (61%)]\tLoss: 0.001324\n",
      " Train Epoch: 6/32 [70528/105216 (67%)]\tLoss: 0.002243\n",
      " Train Epoch: 6/32 [76928/105216 (73%)]\tLoss: 0.002259\n",
      " Train Epoch: 6/32 [83328/105216 (79%)]\tLoss: 0.001075\n",
      " Train Epoch: 6/32 [89728/105216 (85%)]\tLoss: 0.002156\n",
      " Train Epoch: 6/32 [96128/105216 (91%)]\tLoss: 0.001217\n",
      " Train Epoch: 6/32 [102528/105216 (97%)]\tLoss: 0.001788\n",
      " Train Epoch: 7/32 [128/105216 (0%)]\tLoss: 0.002753\n",
      " Train Epoch: 7/32 [6528/105216 (6%)]\tLoss: 0.001055\n",
      " Train Epoch: 7/32 [12928/105216 (12%)]\tLoss: 0.001134\n",
      " Train Epoch: 7/32 [19328/105216 (18%)]\tLoss: 0.000851\n",
      " Train Epoch: 7/32 [25728/105216 (24%)]\tLoss: 0.000933\n",
      " Train Epoch: 7/32 [32128/105216 (31%)]\tLoss: 0.000678\n",
      " Train Epoch: 7/32 [38528/105216 (37%)]\tLoss: 0.000878\n",
      " Train Epoch: 7/32 [44928/105216 (43%)]\tLoss: 0.001836\n",
      " Train Epoch: 7/32 [51328/105216 (49%)]\tLoss: 0.001020\n",
      " Train Epoch: 7/32 [57728/105216 (55%)]\tLoss: 0.000798\n",
      " Train Epoch: 7/32 [64128/105216 (61%)]\tLoss: 0.001055\n",
      " Train Epoch: 7/32 [70528/105216 (67%)]\tLoss: 0.001920\n",
      " Train Epoch: 7/32 [76928/105216 (73%)]\tLoss: 0.001960\n",
      " Train Epoch: 7/32 [83328/105216 (79%)]\tLoss: 0.000992\n",
      " Train Epoch: 7/32 [89728/105216 (85%)]\tLoss: 0.001928\n",
      " Train Epoch: 7/32 [96128/105216 (91%)]\tLoss: 0.001127\n",
      " Train Epoch: 7/32 [102528/105216 (97%)]\tLoss: 0.001527\n",
      " Train Epoch: 8/32 [128/105216 (0%)]\tLoss: 0.002362\n",
      " Train Epoch: 8/32 [6528/105216 (6%)]\tLoss: 0.000984\n",
      " Train Epoch: 8/32 [12928/105216 (12%)]\tLoss: 0.001087\n",
      " Train Epoch: 8/32 [19328/105216 (18%)]\tLoss: 0.000790\n",
      " Train Epoch: 8/32 [25728/105216 (24%)]\tLoss: 0.000851\n",
      " Train Epoch: 8/32 [32128/105216 (31%)]\tLoss: 0.000641\n",
      " Train Epoch: 8/32 [38528/105216 (37%)]\tLoss: 0.000823\n",
      " Train Epoch: 8/32 [44928/105216 (43%)]\tLoss: 0.001767\n",
      " Train Epoch: 8/32 [51328/105216 (49%)]\tLoss: 0.000943\n",
      " Train Epoch: 8/32 [57728/105216 (55%)]\tLoss: 0.000752\n",
      " Train Epoch: 8/32 [64128/105216 (61%)]\tLoss: 0.000929\n",
      " Train Epoch: 8/32 [70528/105216 (67%)]\tLoss: 0.001747\n",
      " Train Epoch: 8/32 [76928/105216 (73%)]\tLoss: 0.001805\n",
      " Train Epoch: 8/32 [83328/105216 (79%)]\tLoss: 0.000936\n",
      " Train Epoch: 8/32 [89728/105216 (85%)]\tLoss: 0.001806\n",
      " Train Epoch: 8/32 [96128/105216 (91%)]\tLoss: 0.001085\n",
      " Train Epoch: 8/32 [102528/105216 (97%)]\tLoss: 0.001414\n",
      " Train Epoch: 9/32 [128/105216 (0%)]\tLoss: 0.002193\n",
      " Train Epoch: 9/32 [6528/105216 (6%)]\tLoss: 0.000936\n",
      " Train Epoch: 9/32 [12928/105216 (12%)]\tLoss: 0.001041\n",
      " Train Epoch: 9/32 [19328/105216 (18%)]\tLoss: 0.000755\n",
      " Train Epoch: 9/32 [25728/105216 (24%)]\tLoss: 0.000810\n",
      " Train Epoch: 9/32 [32128/105216 (31%)]\tLoss: 0.000620\n",
      " Train Epoch: 9/32 [38528/105216 (37%)]\tLoss: 0.000800\n",
      " Train Epoch: 9/32 [44928/105216 (43%)]\tLoss: 0.001718\n",
      " Train Epoch: 9/32 [51328/105216 (49%)]\tLoss: 0.000902\n",
      " Train Epoch: 9/32 [57728/105216 (55%)]\tLoss: 0.000728\n",
      " Train Epoch: 9/32 [64128/105216 (61%)]\tLoss: 0.000853\n",
      " Train Epoch: 9/32 [70528/105216 (67%)]\tLoss: 0.001622\n",
      " Train Epoch: 9/32 [76928/105216 (73%)]\tLoss: 0.001703\n",
      " Train Epoch: 9/32 [83328/105216 (79%)]\tLoss: 0.000885\n",
      " Train Epoch: 9/32 [89728/105216 (85%)]\tLoss: 0.001715\n",
      " Train Epoch: 9/32 [96128/105216 (91%)]\tLoss: 0.001059\n",
      " Train Epoch: 9/32 [102528/105216 (97%)]\tLoss: 0.001363\n",
      " Train Epoch: 10/32 [128/105216 (0%)]\tLoss: 0.002073\n",
      " Train Epoch: 10/32 [6528/105216 (6%)]\tLoss: 0.000897\n",
      " Train Epoch: 10/32 [12928/105216 (12%)]\tLoss: 0.000995\n",
      " Train Epoch: 10/32 [19328/105216 (18%)]\tLoss: 0.000735\n",
      " Train Epoch: 10/32 [25728/105216 (24%)]\tLoss: 0.000780\n",
      " Train Epoch: 10/32 [32128/105216 (31%)]\tLoss: 0.000609\n",
      " Train Epoch: 10/32 [38528/105216 (37%)]\tLoss: 0.000778\n",
      " Train Epoch: 10/32 [44928/105216 (43%)]\tLoss: 0.001653\n",
      " Train Epoch: 10/32 [51328/105216 (49%)]\tLoss: 0.000866\n",
      " Train Epoch: 10/32 [57728/105216 (55%)]\tLoss: 0.000714\n",
      " Train Epoch: 10/32 [64128/105216 (61%)]\tLoss: 0.000790\n",
      " Train Epoch: 10/32 [70528/105216 (67%)]\tLoss: 0.001501\n",
      " Train Epoch: 10/32 [76928/105216 (73%)]\tLoss: 0.001611\n",
      " Train Epoch: 10/32 [83328/105216 (79%)]\tLoss: 0.000852\n",
      " Train Epoch: 10/32 [89728/105216 (85%)]\tLoss: 0.001664\n",
      " Train Epoch: 10/32 [96128/105216 (91%)]\tLoss: 0.001042\n",
      " Train Epoch: 10/32 [102528/105216 (97%)]\tLoss: 0.001364\n",
      " Train Epoch: 11/32 [128/105216 (0%)]\tLoss: 0.001931\n",
      " Train Epoch: 11/32 [6528/105216 (6%)]\tLoss: 0.000866\n",
      " Train Epoch: 11/32 [12928/105216 (12%)]\tLoss: 0.000960\n",
      " Train Epoch: 11/32 [19328/105216 (18%)]\tLoss: 0.000724\n",
      " Train Epoch: 11/32 [25728/105216 (24%)]\tLoss: 0.000765\n",
      " Train Epoch: 11/32 [32128/105216 (31%)]\tLoss: 0.000604\n",
      " Train Epoch: 11/32 [38528/105216 (37%)]\tLoss: 0.000749\n",
      " Train Epoch: 11/32 [44928/105216 (43%)]\tLoss: 0.001567\n",
      " Train Epoch: 11/32 [51328/105216 (49%)]\tLoss: 0.000847\n",
      " Train Epoch: 11/32 [57728/105216 (55%)]\tLoss: 0.000703\n",
      " Train Epoch: 11/32 [64128/105216 (61%)]\tLoss: 0.000742\n",
      " Train Epoch: 11/32 [70528/105216 (67%)]\tLoss: 0.001385\n",
      " Train Epoch: 11/32 [76928/105216 (73%)]\tLoss: 0.001544\n",
      " Train Epoch: 11/32 [83328/105216 (79%)]\tLoss: 0.000824\n",
      " Train Epoch: 11/32 [89728/105216 (85%)]\tLoss: 0.001613\n",
      " Train Epoch: 11/32 [96128/105216 (91%)]\tLoss: 0.001028\n",
      " Train Epoch: 11/32 [102528/105216 (97%)]\tLoss: 0.001372\n",
      " Train Epoch: 12/32 [128/105216 (0%)]\tLoss: 0.001796\n",
      " Train Epoch: 12/32 [6528/105216 (6%)]\tLoss: 0.000836\n",
      " Train Epoch: 12/32 [12928/105216 (12%)]\tLoss: 0.000932\n",
      " Train Epoch: 12/32 [19328/105216 (18%)]\tLoss: 0.000711\n",
      " Train Epoch: 12/32 [25728/105216 (24%)]\tLoss: 0.000754\n",
      " Train Epoch: 12/32 [32128/105216 (31%)]\tLoss: 0.000605\n",
      " Train Epoch: 12/32 [38528/105216 (37%)]\tLoss: 0.000721\n",
      " Train Epoch: 12/32 [44928/105216 (43%)]\tLoss: 0.001490\n",
      " Train Epoch: 12/32 [51328/105216 (49%)]\tLoss: 0.000833\n",
      " Train Epoch: 12/32 [57728/105216 (55%)]\tLoss: 0.000697\n",
      " Train Epoch: 12/32 [64128/105216 (61%)]\tLoss: 0.000701\n",
      " Train Epoch: 12/32 [70528/105216 (67%)]\tLoss: 0.001294\n",
      " Train Epoch: 12/32 [76928/105216 (73%)]\tLoss: 0.001493\n",
      " Train Epoch: 12/32 [83328/105216 (79%)]\tLoss: 0.000799\n",
      " Train Epoch: 12/32 [89728/105216 (85%)]\tLoss: 0.001577\n",
      " Train Epoch: 12/32 [96128/105216 (91%)]\tLoss: 0.001015\n",
      " Train Epoch: 12/32 [102528/105216 (97%)]\tLoss: 0.001384\n",
      " Train Epoch: 13/32 [128/105216 (0%)]\tLoss: 0.001717\n",
      " Train Epoch: 13/32 [6528/105216 (6%)]\tLoss: 0.000803\n",
      " Train Epoch: 13/32 [12928/105216 (12%)]\tLoss: 0.000910\n",
      " Train Epoch: 13/32 [19328/105216 (18%)]\tLoss: 0.000701\n",
      " Train Epoch: 13/32 [25728/105216 (24%)]\tLoss: 0.000744\n",
      " Train Epoch: 13/32 [32128/105216 (31%)]\tLoss: 0.000600\n",
      " Train Epoch: 13/32 [38528/105216 (37%)]\tLoss: 0.000696\n",
      " Train Epoch: 13/32 [44928/105216 (43%)]\tLoss: 0.001434\n",
      " Train Epoch: 13/32 [51328/105216 (49%)]\tLoss: 0.000822\n",
      " Train Epoch: 13/32 [57728/105216 (55%)]\tLoss: 0.000692\n",
      " Train Epoch: 13/32 [64128/105216 (61%)]\tLoss: 0.000662\n",
      " Train Epoch: 13/32 [70528/105216 (67%)]\tLoss: 0.001226\n",
      " Train Epoch: 13/32 [76928/105216 (73%)]\tLoss: 0.001452\n",
      " Train Epoch: 13/32 [83328/105216 (79%)]\tLoss: 0.000781\n",
      " Train Epoch: 13/32 [89728/105216 (85%)]\tLoss: 0.001549\n",
      " Train Epoch: 13/32 [96128/105216 (91%)]\tLoss: 0.001002\n",
      " Train Epoch: 13/32 [102528/105216 (97%)]\tLoss: 0.001373\n",
      " Train Epoch: 14/32 [128/105216 (0%)]\tLoss: 0.001630\n",
      " Train Epoch: 14/32 [6528/105216 (6%)]\tLoss: 0.000776\n",
      " Train Epoch: 14/32 [12928/105216 (12%)]\tLoss: 0.000892\n",
      " Train Epoch: 14/32 [19328/105216 (18%)]\tLoss: 0.000692\n",
      " Train Epoch: 14/32 [25728/105216 (24%)]\tLoss: 0.000733\n",
      " Train Epoch: 14/32 [32128/105216 (31%)]\tLoss: 0.000599\n",
      " Train Epoch: 14/32 [38528/105216 (37%)]\tLoss: 0.000670\n",
      " Train Epoch: 14/32 [44928/105216 (43%)]\tLoss: 0.001369\n",
      " Train Epoch: 14/32 [51328/105216 (49%)]\tLoss: 0.000814\n",
      " Train Epoch: 14/32 [57728/105216 (55%)]\tLoss: 0.000683\n",
      " Train Epoch: 14/32 [64128/105216 (61%)]\tLoss: 0.000634\n",
      " Train Epoch: 14/32 [70528/105216 (67%)]\tLoss: 0.001166\n",
      " Train Epoch: 14/32 [76928/105216 (73%)]\tLoss: 0.001416\n",
      " Train Epoch: 14/32 [83328/105216 (79%)]\tLoss: 0.000764\n",
      " Train Epoch: 14/32 [89728/105216 (85%)]\tLoss: 0.001520\n",
      " Train Epoch: 14/32 [96128/105216 (91%)]\tLoss: 0.000992\n",
      " Train Epoch: 14/32 [102528/105216 (97%)]\tLoss: 0.001401\n",
      " Train Epoch: 15/32 [128/105216 (0%)]\tLoss: 0.001561\n",
      " Train Epoch: 15/32 [6528/105216 (6%)]\tLoss: 0.000751\n",
      " Train Epoch: 15/32 [12928/105216 (12%)]\tLoss: 0.000873\n",
      " Train Epoch: 15/32 [19328/105216 (18%)]\tLoss: 0.000680\n",
      " Train Epoch: 15/32 [25728/105216 (24%)]\tLoss: 0.000724\n",
      " Train Epoch: 15/32 [32128/105216 (31%)]\tLoss: 0.000601\n",
      " Train Epoch: 15/32 [38528/105216 (37%)]\tLoss: 0.000664\n",
      " Train Epoch: 15/32 [44928/105216 (43%)]\tLoss: 0.001313\n",
      " Train Epoch: 15/32 [51328/105216 (49%)]\tLoss: 0.000804\n",
      " Train Epoch: 15/32 [57728/105216 (55%)]\tLoss: 0.000675\n",
      " Train Epoch: 15/32 [64128/105216 (61%)]\tLoss: 0.000614\n",
      " Train Epoch: 15/32 [70528/105216 (67%)]\tLoss: 0.001115\n",
      " Train Epoch: 15/32 [76928/105216 (73%)]\tLoss: 0.001393\n",
      " Train Epoch: 15/32 [83328/105216 (79%)]\tLoss: 0.000748\n",
      " Train Epoch: 15/32 [89728/105216 (85%)]\tLoss: 0.001494\n",
      " Train Epoch: 15/32 [96128/105216 (91%)]\tLoss: 0.000981\n",
      " Train Epoch: 15/32 [102528/105216 (97%)]\tLoss: 0.001409\n",
      " Train Epoch: 16/32 [128/105216 (0%)]\tLoss: 0.001505\n",
      " Train Epoch: 16/32 [6528/105216 (6%)]\tLoss: 0.000733\n",
      " Train Epoch: 16/32 [12928/105216 (12%)]\tLoss: 0.000861\n",
      " Train Epoch: 16/32 [19328/105216 (18%)]\tLoss: 0.000671\n",
      " Train Epoch: 16/32 [25728/105216 (24%)]\tLoss: 0.000711\n",
      " Train Epoch: 16/32 [32128/105216 (31%)]\tLoss: 0.000610\n",
      " Train Epoch: 16/32 [38528/105216 (37%)]\tLoss: 0.000661\n",
      " Train Epoch: 16/32 [44928/105216 (43%)]\tLoss: 0.001219\n",
      " Train Epoch: 16/32 [51328/105216 (49%)]\tLoss: 0.000795\n",
      " Train Epoch: 16/32 [57728/105216 (55%)]\tLoss: 0.000667\n",
      " Train Epoch: 16/32 [64128/105216 (61%)]\tLoss: 0.000586\n",
      " Train Epoch: 16/32 [70528/105216 (67%)]\tLoss: 0.001062\n",
      " Train Epoch: 16/32 [76928/105216 (73%)]\tLoss: 0.001364\n",
      " Train Epoch: 16/32 [83328/105216 (79%)]\tLoss: 0.000736\n",
      " Train Epoch: 16/32 [89728/105216 (85%)]\tLoss: 0.001473\n",
      " Train Epoch: 16/32 [96128/105216 (91%)]\tLoss: 0.000964\n",
      " Train Epoch: 16/32 [102528/105216 (97%)]\tLoss: 0.001373\n",
      " Train Epoch: 17/32 [128/105216 (0%)]\tLoss: 0.001408\n",
      " Train Epoch: 17/32 [6528/105216 (6%)]\tLoss: 0.000699\n",
      " Train Epoch: 17/32 [12928/105216 (12%)]\tLoss: 0.000833\n",
      " Train Epoch: 17/32 [19328/105216 (18%)]\tLoss: 0.000661\n",
      " Train Epoch: 17/32 [25728/105216 (24%)]\tLoss: 0.000700\n",
      " Train Epoch: 17/32 [32128/105216 (31%)]\tLoss: 0.000611\n",
      " Train Epoch: 17/32 [38528/105216 (37%)]\tLoss: 0.000678\n",
      " Train Epoch: 17/32 [44928/105216 (43%)]\tLoss: 0.001116\n",
      " Train Epoch: 17/32 [51328/105216 (49%)]\tLoss: 0.000776\n",
      " Train Epoch: 17/32 [57728/105216 (55%)]\tLoss: 0.000658\n",
      " Train Epoch: 17/32 [64128/105216 (61%)]\tLoss: 0.000567\n",
      " Train Epoch: 17/32 [70528/105216 (67%)]\tLoss: 0.001023\n",
      " Train Epoch: 17/32 [76928/105216 (73%)]\tLoss: 0.001354\n",
      " Train Epoch: 17/32 [83328/105216 (79%)]\tLoss: 0.000715\n",
      " Train Epoch: 17/32 [89728/105216 (85%)]\tLoss: 0.001449\n",
      " Train Epoch: 17/32 [96128/105216 (91%)]\tLoss: 0.000946\n",
      " Train Epoch: 17/32 [102528/105216 (97%)]\tLoss: 0.001310\n",
      " Train Epoch: 18/32 [128/105216 (0%)]\tLoss: 0.001335\n",
      " Train Epoch: 18/32 [6528/105216 (6%)]\tLoss: 0.000673\n",
      " Train Epoch: 18/32 [12928/105216 (12%)]\tLoss: 0.000813\n",
      " Train Epoch: 18/32 [19328/105216 (18%)]\tLoss: 0.000648\n",
      " Train Epoch: 18/32 [25728/105216 (24%)]\tLoss: 0.000689\n",
      " Train Epoch: 18/32 [32128/105216 (31%)]\tLoss: 0.000603\n",
      " Train Epoch: 18/32 [38528/105216 (37%)]\tLoss: 0.000680\n",
      " Train Epoch: 18/32 [44928/105216 (43%)]\tLoss: 0.001030\n",
      " Train Epoch: 18/32 [51328/105216 (49%)]\tLoss: 0.000752\n",
      " Train Epoch: 18/32 [57728/105216 (55%)]\tLoss: 0.000645\n",
      " Train Epoch: 18/32 [64128/105216 (61%)]\tLoss: 0.000555\n",
      " Train Epoch: 18/32 [70528/105216 (67%)]\tLoss: 0.000991\n",
      " Train Epoch: 18/32 [76928/105216 (73%)]\tLoss: 0.001301\n",
      " Train Epoch: 18/32 [83328/105216 (79%)]\tLoss: 0.000693\n",
      " Train Epoch: 18/32 [89728/105216 (85%)]\tLoss: 0.001428\n",
      " Train Epoch: 18/32 [96128/105216 (91%)]\tLoss: 0.000923\n",
      " Train Epoch: 18/32 [102528/105216 (97%)]\tLoss: 0.001216\n",
      " Train Epoch: 19/32 [128/105216 (0%)]\tLoss: 0.001264\n",
      " Train Epoch: 19/32 [6528/105216 (6%)]\tLoss: 0.000662\n",
      " Train Epoch: 19/32 [12928/105216 (12%)]\tLoss: 0.000791\n",
      " Train Epoch: 19/32 [19328/105216 (18%)]\tLoss: 0.000634\n",
      " Train Epoch: 19/32 [25728/105216 (24%)]\tLoss: 0.000678\n",
      " Train Epoch: 19/32 [32128/105216 (31%)]\tLoss: 0.000582\n",
      " Train Epoch: 19/32 [38528/105216 (37%)]\tLoss: 0.000682\n",
      " Train Epoch: 19/32 [44928/105216 (43%)]\tLoss: 0.000976\n",
      " Train Epoch: 19/32 [51328/105216 (49%)]\tLoss: 0.000730\n",
      " Train Epoch: 19/32 [57728/105216 (55%)]\tLoss: 0.000629\n",
      " Train Epoch: 19/32 [64128/105216 (61%)]\tLoss: 0.000540\n",
      " Train Epoch: 19/32 [70528/105216 (67%)]\tLoss: 0.000926\n",
      " Train Epoch: 19/32 [76928/105216 (73%)]\tLoss: 0.001250\n",
      " Train Epoch: 19/32 [83328/105216 (79%)]\tLoss: 0.000662\n",
      " Train Epoch: 19/32 [89728/105216 (85%)]\tLoss: 0.001398\n",
      " Train Epoch: 19/32 [96128/105216 (91%)]\tLoss: 0.000898\n",
      " Train Epoch: 19/32 [102528/105216 (97%)]\tLoss: 0.001115\n",
      " Train Epoch: 20/32 [128/105216 (0%)]\tLoss: 0.001156\n",
      " Train Epoch: 20/32 [6528/105216 (6%)]\tLoss: 0.000628\n",
      " Train Epoch: 20/32 [12928/105216 (12%)]\tLoss: 0.000769\n",
      " Train Epoch: 20/32 [19328/105216 (18%)]\tLoss: 0.000617\n",
      " Train Epoch: 20/32 [25728/105216 (24%)]\tLoss: 0.000661\n",
      " Train Epoch: 20/32 [32128/105216 (31%)]\tLoss: 0.000554\n",
      " Train Epoch: 20/32 [38528/105216 (37%)]\tLoss: 0.000686\n",
      " Train Epoch: 20/32 [44928/105216 (43%)]\tLoss: 0.000921\n",
      " Train Epoch: 20/32 [51328/105216 (49%)]\tLoss: 0.000707\n",
      " Train Epoch: 20/32 [57728/105216 (55%)]\tLoss: 0.000615\n",
      " Train Epoch: 20/32 [64128/105216 (61%)]\tLoss: 0.000530\n",
      " Train Epoch: 20/32 [70528/105216 (67%)]\tLoss: 0.000859\n",
      " Train Epoch: 20/32 [76928/105216 (73%)]\tLoss: 0.001228\n",
      " Train Epoch: 20/32 [83328/105216 (79%)]\tLoss: 0.000641\n",
      " Train Epoch: 20/32 [89728/105216 (85%)]\tLoss: 0.001360\n",
      " Train Epoch: 20/32 [96128/105216 (91%)]\tLoss: 0.000870\n",
      " Train Epoch: 20/32 [102528/105216 (97%)]\tLoss: 0.001009\n",
      " Train Epoch: 21/32 [128/105216 (0%)]\tLoss: 0.001061\n",
      " Train Epoch: 21/32 [6528/105216 (6%)]\tLoss: 0.000598\n",
      " Train Epoch: 21/32 [12928/105216 (12%)]\tLoss: 0.000748\n",
      " Train Epoch: 21/32 [19328/105216 (18%)]\tLoss: 0.000593\n",
      " Train Epoch: 21/32 [25728/105216 (24%)]\tLoss: 0.000643\n",
      " Train Epoch: 21/32 [32128/105216 (31%)]\tLoss: 0.000529\n",
      " Train Epoch: 21/32 [38528/105216 (37%)]\tLoss: 0.000677\n",
      " Train Epoch: 21/32 [44928/105216 (43%)]\tLoss: 0.000884\n",
      " Train Epoch: 21/32 [51328/105216 (49%)]\tLoss: 0.000688\n",
      " Train Epoch: 21/32 [57728/105216 (55%)]\tLoss: 0.000595\n",
      " Train Epoch: 21/32 [64128/105216 (61%)]\tLoss: 0.000526\n",
      " Train Epoch: 21/32 [70528/105216 (67%)]\tLoss: 0.000815\n",
      " Train Epoch: 21/32 [76928/105216 (73%)]\tLoss: 0.001192\n",
      " Train Epoch: 21/32 [83328/105216 (79%)]\tLoss: 0.000612\n",
      " Train Epoch: 21/32 [89728/105216 (85%)]\tLoss: 0.001312\n",
      " Train Epoch: 21/32 [96128/105216 (91%)]\tLoss: 0.000837\n",
      " Train Epoch: 21/32 [102528/105216 (97%)]\tLoss: 0.000914\n",
      " Train Epoch: 22/32 [128/105216 (0%)]\tLoss: 0.000997\n",
      " Train Epoch: 22/32 [6528/105216 (6%)]\tLoss: 0.000519\n",
      " Train Epoch: 22/32 [12928/105216 (12%)]\tLoss: 0.000714\n",
      " Train Epoch: 22/32 [19328/105216 (18%)]\tLoss: 0.000566\n",
      " Train Epoch: 22/32 [25728/105216 (24%)]\tLoss: 0.000623\n",
      " Train Epoch: 22/32 [32128/105216 (31%)]\tLoss: 0.000500\n",
      " Train Epoch: 22/32 [38528/105216 (37%)]\tLoss: 0.000663\n",
      " Train Epoch: 22/32 [44928/105216 (43%)]\tLoss: 0.000848\n",
      " Train Epoch: 22/32 [51328/105216 (49%)]\tLoss: 0.000664\n",
      " Train Epoch: 22/32 [57728/105216 (55%)]\tLoss: 0.000564\n",
      " Train Epoch: 22/32 [64128/105216 (61%)]\tLoss: 0.000523\n",
      " Train Epoch: 22/32 [70528/105216 (67%)]\tLoss: 0.000774\n",
      " Train Epoch: 22/32 [76928/105216 (73%)]\tLoss: 0.001161\n",
      " Train Epoch: 22/32 [83328/105216 (79%)]\tLoss: 0.000582\n",
      " Train Epoch: 22/32 [89728/105216 (85%)]\tLoss: 0.001245\n",
      " Train Epoch: 22/32 [96128/105216 (91%)]\tLoss: 0.000794\n",
      " Train Epoch: 22/32 [102528/105216 (97%)]\tLoss: 0.000817\n",
      " Train Epoch: 23/32 [128/105216 (0%)]\tLoss: 0.000925\n",
      " Train Epoch: 23/32 [6528/105216 (6%)]\tLoss: 0.000454\n",
      " Train Epoch: 23/32 [12928/105216 (12%)]\tLoss: 0.000664\n",
      " Train Epoch: 23/32 [19328/105216 (18%)]\tLoss: 0.000525\n",
      " Train Epoch: 23/32 [25728/105216 (24%)]\tLoss: 0.000593\n",
      " Train Epoch: 23/32 [32128/105216 (31%)]\tLoss: 0.000474\n",
      " Train Epoch: 23/32 [38528/105216 (37%)]\tLoss: 0.000637\n",
      " Train Epoch: 23/32 [44928/105216 (43%)]\tLoss: 0.000777\n",
      " Train Epoch: 23/32 [51328/105216 (49%)]\tLoss: 0.000615\n",
      " Train Epoch: 23/32 [57728/105216 (55%)]\tLoss: 0.000518\n",
      " Train Epoch: 23/32 [64128/105216 (61%)]\tLoss: 0.000506\n",
      " Train Epoch: 23/32 [70528/105216 (67%)]\tLoss: 0.000753\n",
      " Train Epoch: 23/32 [76928/105216 (73%)]\tLoss: 0.001120\n",
      " Train Epoch: 23/32 [83328/105216 (79%)]\tLoss: 0.000523\n",
      " Train Epoch: 23/32 [89728/105216 (85%)]\tLoss: 0.001167\n",
      " Train Epoch: 23/32 [96128/105216 (91%)]\tLoss: 0.000729\n",
      " Train Epoch: 23/32 [102528/105216 (97%)]\tLoss: 0.000762\n",
      " Train Epoch: 24/32 [128/105216 (0%)]\tLoss: 0.000880\n",
      " Train Epoch: 24/32 [6528/105216 (6%)]\tLoss: 0.000398\n",
      " Train Epoch: 24/32 [12928/105216 (12%)]\tLoss: 0.000593\n",
      " Train Epoch: 24/32 [19328/105216 (18%)]\tLoss: 0.000468\n",
      " Train Epoch: 24/32 [25728/105216 (24%)]\tLoss: 0.000554\n",
      " Train Epoch: 24/32 [32128/105216 (31%)]\tLoss: 0.000457\n",
      " Train Epoch: 24/32 [38528/105216 (37%)]\tLoss: 0.000611\n",
      " Train Epoch: 24/32 [44928/105216 (43%)]\tLoss: 0.000694\n",
      " Train Epoch: 24/32 [51328/105216 (49%)]\tLoss: 0.000561\n",
      " Train Epoch: 24/32 [57728/105216 (55%)]\tLoss: 0.000462\n",
      " Train Epoch: 24/32 [64128/105216 (61%)]\tLoss: 0.000490\n",
      " Train Epoch: 24/32 [70528/105216 (67%)]\tLoss: 0.000714\n",
      " Train Epoch: 24/32 [76928/105216 (73%)]\tLoss: 0.001135\n",
      " Train Epoch: 24/32 [83328/105216 (79%)]\tLoss: 0.000456\n",
      " Train Epoch: 24/32 [89728/105216 (85%)]\tLoss: 0.001050\n",
      " Train Epoch: 24/32 [96128/105216 (91%)]\tLoss: 0.000645\n",
      " Train Epoch: 24/32 [102528/105216 (97%)]\tLoss: 0.000716\n",
      " Train Epoch: 25/32 [128/105216 (0%)]\tLoss: 0.000831\n",
      " Train Epoch: 25/32 [6528/105216 (6%)]\tLoss: 0.000351\n",
      " Train Epoch: 25/32 [12928/105216 (12%)]\tLoss: 0.000509\n",
      " Train Epoch: 25/32 [19328/105216 (18%)]\tLoss: 0.000406\n",
      " Train Epoch: 25/32 [25728/105216 (24%)]\tLoss: 0.000501\n",
      " Train Epoch: 25/32 [32128/105216 (31%)]\tLoss: 0.000441\n",
      " Train Epoch: 25/32 [38528/105216 (37%)]\tLoss: 0.000596\n",
      " Train Epoch: 25/32 [44928/105216 (43%)]\tLoss: 0.000615\n",
      " Train Epoch: 25/32 [51328/105216 (49%)]\tLoss: 0.000511\n",
      " Train Epoch: 25/32 [57728/105216 (55%)]\tLoss: 0.000399\n",
      " Train Epoch: 25/32 [64128/105216 (61%)]\tLoss: 0.000471\n",
      " Train Epoch: 25/32 [70528/105216 (67%)]\tLoss: 0.000688\n",
      " Train Epoch: 25/32 [76928/105216 (73%)]\tLoss: 0.001125\n",
      " Train Epoch: 25/32 [83328/105216 (79%)]\tLoss: 0.000399\n",
      " Train Epoch: 25/32 [89728/105216 (85%)]\tLoss: 0.000953\n",
      " Train Epoch: 25/32 [96128/105216 (91%)]\tLoss: 0.000553\n",
      " Train Epoch: 25/32 [102528/105216 (97%)]\tLoss: 0.000698\n",
      " Train Epoch: 26/32 [128/105216 (0%)]\tLoss: 0.000806\n",
      " Train Epoch: 26/32 [6528/105216 (6%)]\tLoss: 0.000344\n",
      " Train Epoch: 26/32 [12928/105216 (12%)]\tLoss: 0.000440\n",
      " Train Epoch: 26/32 [19328/105216 (18%)]\tLoss: 0.000354\n",
      " Train Epoch: 26/32 [25728/105216 (24%)]\tLoss: 0.000449\n",
      " Train Epoch: 26/32 [32128/105216 (31%)]\tLoss: 0.000433\n",
      " Train Epoch: 26/32 [38528/105216 (37%)]\tLoss: 0.000591\n",
      " Train Epoch: 26/32 [44928/105216 (43%)]\tLoss: 0.000542\n",
      " Train Epoch: 26/32 [51328/105216 (49%)]\tLoss: 0.000471\n",
      " Train Epoch: 26/32 [57728/105216 (55%)]\tLoss: 0.000344\n",
      " Train Epoch: 26/32 [64128/105216 (61%)]\tLoss: 0.000464\n",
      " Train Epoch: 26/32 [70528/105216 (67%)]\tLoss: 0.000668\n",
      " Train Epoch: 26/32 [76928/105216 (73%)]\tLoss: 0.001072\n",
      " Train Epoch: 26/32 [83328/105216 (79%)]\tLoss: 0.000356\n",
      " Train Epoch: 26/32 [89728/105216 (85%)]\tLoss: 0.000872\n",
      " Train Epoch: 26/32 [96128/105216 (91%)]\tLoss: 0.000481\n",
      " Train Epoch: 26/32 [102528/105216 (97%)]\tLoss: 0.000675\n",
      " Train Epoch: 27/32 [128/105216 (0%)]\tLoss: 0.000806\n",
      " Train Epoch: 27/32 [6528/105216 (6%)]\tLoss: 0.000336\n",
      " Train Epoch: 27/32 [12928/105216 (12%)]\tLoss: 0.000399\n",
      " Train Epoch: 27/32 [19328/105216 (18%)]\tLoss: 0.000315\n",
      " Train Epoch: 27/32 [25728/105216 (24%)]\tLoss: 0.000410\n",
      " Train Epoch: 27/32 [32128/105216 (31%)]\tLoss: 0.000422\n",
      " Train Epoch: 27/32 [38528/105216 (37%)]\tLoss: 0.000597\n",
      " Train Epoch: 27/32 [44928/105216 (43%)]\tLoss: 0.000487\n",
      " Train Epoch: 27/32 [51328/105216 (49%)]\tLoss: 0.000446\n",
      " Train Epoch: 27/32 [57728/105216 (55%)]\tLoss: 0.000307\n",
      " Train Epoch: 27/32 [64128/105216 (61%)]\tLoss: 0.000463\n",
      " Train Epoch: 27/32 [70528/105216 (67%)]\tLoss: 0.000657\n",
      " Train Epoch: 27/32 [76928/105216 (73%)]\tLoss: 0.001043\n",
      " Train Epoch: 27/32 [83328/105216 (79%)]\tLoss: 0.000326\n",
      " Train Epoch: 27/32 [89728/105216 (85%)]\tLoss: 0.000829\n",
      " Train Epoch: 27/32 [96128/105216 (91%)]\tLoss: 0.000437\n",
      " Train Epoch: 27/32 [102528/105216 (97%)]\tLoss: 0.000664\n",
      " Train Epoch: 28/32 [128/105216 (0%)]\tLoss: 0.000798\n",
      " Train Epoch: 28/32 [6528/105216 (6%)]\tLoss: 0.000331\n",
      " Train Epoch: 28/32 [12928/105216 (12%)]\tLoss: 0.000366\n",
      " Train Epoch: 28/32 [19328/105216 (18%)]\tLoss: 0.000290\n",
      " Train Epoch: 28/32 [25728/105216 (24%)]\tLoss: 0.000386\n",
      " Train Epoch: 28/32 [32128/105216 (31%)]\tLoss: 0.000416\n",
      " Train Epoch: 28/32 [38528/105216 (37%)]\tLoss: 0.000609\n",
      " Train Epoch: 28/32 [44928/105216 (43%)]\tLoss: 0.000449\n",
      " Train Epoch: 28/32 [51328/105216 (49%)]\tLoss: 0.000426\n",
      " Train Epoch: 28/32 [57728/105216 (55%)]\tLoss: 0.000281\n",
      " Train Epoch: 28/32 [64128/105216 (61%)]\tLoss: 0.000466\n",
      " Train Epoch: 28/32 [70528/105216 (67%)]\tLoss: 0.000650\n",
      " Train Epoch: 28/32 [76928/105216 (73%)]\tLoss: 0.001014\n",
      " Train Epoch: 28/32 [83328/105216 (79%)]\tLoss: 0.000303\n",
      " Train Epoch: 28/32 [89728/105216 (85%)]\tLoss: 0.000802\n",
      " Train Epoch: 28/32 [96128/105216 (91%)]\tLoss: 0.000411\n",
      " Train Epoch: 28/32 [102528/105216 (97%)]\tLoss: 0.000657\n",
      " Train Epoch: 29/32 [128/105216 (0%)]\tLoss: 0.000778\n",
      " Train Epoch: 29/32 [6528/105216 (6%)]\tLoss: 0.000324\n",
      " Train Epoch: 29/32 [12928/105216 (12%)]\tLoss: 0.000344\n",
      " Train Epoch: 29/32 [19328/105216 (18%)]\tLoss: 0.000273\n",
      " Train Epoch: 29/32 [25728/105216 (24%)]\tLoss: 0.000368\n",
      " Train Epoch: 29/32 [32128/105216 (31%)]\tLoss: 0.000412\n",
      " Train Epoch: 29/32 [38528/105216 (37%)]\tLoss: 0.000614\n",
      " Train Epoch: 29/32 [44928/105216 (43%)]\tLoss: 0.000424\n",
      " Train Epoch: 29/32 [51328/105216 (49%)]\tLoss: 0.000411\n",
      " Train Epoch: 29/32 [57728/105216 (55%)]\tLoss: 0.000265\n",
      " Train Epoch: 29/32 [64128/105216 (61%)]\tLoss: 0.000462\n",
      " Train Epoch: 29/32 [70528/105216 (67%)]\tLoss: 0.000633\n",
      " Train Epoch: 29/32 [76928/105216 (73%)]\tLoss: 0.000992\n",
      " Train Epoch: 29/32 [83328/105216 (79%)]\tLoss: 0.000287\n",
      " Train Epoch: 29/32 [89728/105216 (85%)]\tLoss: 0.000777\n",
      " Train Epoch: 29/32 [96128/105216 (91%)]\tLoss: 0.000395\n",
      " Train Epoch: 29/32 [102528/105216 (97%)]\tLoss: 0.000637\n",
      " Train Epoch: 30/32 [128/105216 (0%)]\tLoss: 0.000762\n",
      " Train Epoch: 30/32 [6528/105216 (6%)]\tLoss: 0.000317\n",
      " Train Epoch: 30/32 [12928/105216 (12%)]\tLoss: 0.000330\n",
      " Train Epoch: 30/32 [19328/105216 (18%)]\tLoss: 0.000264\n",
      " Train Epoch: 30/32 [25728/105216 (24%)]\tLoss: 0.000358\n",
      " Train Epoch: 30/32 [32128/105216 (31%)]\tLoss: 0.000408\n",
      " Train Epoch: 30/32 [38528/105216 (37%)]\tLoss: 0.000614\n",
      " Train Epoch: 30/32 [44928/105216 (43%)]\tLoss: 0.000408\n",
      " Train Epoch: 30/32 [51328/105216 (49%)]\tLoss: 0.000403\n",
      " Train Epoch: 30/32 [57728/105216 (55%)]\tLoss: 0.000256\n",
      " Train Epoch: 30/32 [64128/105216 (61%)]\tLoss: 0.000455\n",
      " Train Epoch: 30/32 [70528/105216 (67%)]\tLoss: 0.000626\n",
      " Train Epoch: 30/32 [76928/105216 (73%)]\tLoss: 0.000969\n",
      " Train Epoch: 30/32 [83328/105216 (79%)]\tLoss: 0.000277\n",
      " Train Epoch: 30/32 [89728/105216 (85%)]\tLoss: 0.000751\n",
      " Train Epoch: 30/32 [96128/105216 (91%)]\tLoss: 0.000387\n",
      " Train Epoch: 30/32 [102528/105216 (97%)]\tLoss: 0.000618\n",
      " Train Epoch: 31/32 [128/105216 (0%)]\tLoss: 0.000754\n",
      " Train Epoch: 31/32 [6528/105216 (6%)]\tLoss: 0.000312\n",
      " Train Epoch: 31/32 [12928/105216 (12%)]\tLoss: 0.000321\n",
      " Train Epoch: 31/32 [19328/105216 (18%)]\tLoss: 0.000260\n",
      " Train Epoch: 31/32 [25728/105216 (24%)]\tLoss: 0.000352\n",
      " Train Epoch: 31/32 [32128/105216 (31%)]\tLoss: 0.000406\n",
      " Train Epoch: 31/32 [38528/105216 (37%)]\tLoss: 0.000610\n",
      " Train Epoch: 31/32 [44928/105216 (43%)]\tLoss: 0.000399\n",
      " Train Epoch: 31/32 [51328/105216 (49%)]\tLoss: 0.000398\n",
      " Train Epoch: 31/32 [57728/105216 (55%)]\tLoss: 0.000251\n",
      " Train Epoch: 31/32 [64128/105216 (61%)]\tLoss: 0.000449\n",
      " Train Epoch: 31/32 [70528/105216 (67%)]\tLoss: 0.000615\n",
      " Train Epoch: 31/32 [76928/105216 (73%)]\tLoss: 0.000951\n",
      " Train Epoch: 31/32 [83328/105216 (79%)]\tLoss: 0.000272\n",
      " Train Epoch: 31/32 [89728/105216 (85%)]\tLoss: 0.000731\n",
      " Train Epoch: 31/32 [96128/105216 (91%)]\tLoss: 0.000383\n",
      " Train Epoch: 31/32 [102528/105216 (97%)]\tLoss: 0.000605\n",
      " Train Epoch: 32/32 [128/105216 (0%)]\tLoss: 0.000744\n",
      " Train Epoch: 32/32 [6528/105216 (6%)]\tLoss: 0.000307\n",
      " Train Epoch: 32/32 [12928/105216 (12%)]\tLoss: 0.000315\n",
      " Train Epoch: 32/32 [19328/105216 (18%)]\tLoss: 0.000257\n",
      " Train Epoch: 32/32 [25728/105216 (24%)]\tLoss: 0.000347\n",
      " Train Epoch: 32/32 [32128/105216 (31%)]\tLoss: 0.000403\n",
      " Train Epoch: 32/32 [38528/105216 (37%)]\tLoss: 0.000606\n",
      " Train Epoch: 32/32 [44928/105216 (43%)]\tLoss: 0.000392\n",
      " Train Epoch: 32/32 [51328/105216 (49%)]\tLoss: 0.000394\n",
      " Train Epoch: 32/32 [57728/105216 (55%)]\tLoss: 0.000247\n",
      " Train Epoch: 32/32 [64128/105216 (61%)]\tLoss: 0.000446\n",
      " Train Epoch: 32/32 [70528/105216 (67%)]\tLoss: 0.000609\n",
      " Train Epoch: 32/32 [76928/105216 (73%)]\tLoss: 0.000932\n",
      " Train Epoch: 32/32 [83328/105216 (79%)]\tLoss: 0.000268\n",
      " Train Epoch: 32/32 [89728/105216 (85%)]\tLoss: 0.000716\n",
      " Train Epoch: 32/32 [96128/105216 (91%)]\tLoss: 0.000377\n",
      " Train Epoch: 32/32 [102528/105216 (97%)]\tLoss: 0.000599"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "EPOCHS = 32\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for batch_idx, (data,target) in enumerate(trn_dataloader):\n",
    "        data = torch.autograd.Variable(data)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(data)\n",
    "        loss = critereon(pred, data)\n",
    "        losses.append(loss.cpu().data.item())\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Display\n",
    "        if batch_idx % 50 == 1:\n",
    "            print('\\n Train Epoch: {}/{} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch+1,\n",
    "                EPOCHS,\n",
    "                batch_idx * len(data), \n",
    "                len(trn_dataloader.dataset),\n",
    "                100. * batch_idx / len(trn_dataloader), \n",
    "                loss.cpu().data.item()), \n",
    "                end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(16,8))\n",
    "plt.plot(np.arange(len(losses)),losses)\n",
    "plt.title('Loss (Training) Einfacher AE',fontsize=20)\n",
    "loss_fn = '{}_AE_model_loss.pdf'.format(arrow.now().format('YYYYMMDD'))\n",
    "fn = os.path.join(fig_path, loss_fn)\n",
    "fig.savefig(fn, bbox_inches = 'tight', pad_inches = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name: 20200303_firstAE_model.pt\n"
     ]
    }
   ],
   "source": [
    "model_fn = '{}_firstAE_model.pt'.format(arrow.now().format('YYYYMMDD'))\n",
    "print('Model Name: {}'.format(model_fn))\n",
    "torch.save(model.state_dict(), os.path.join(model_bib_path, model_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression zum Lernen des Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x130e45990>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from utils.evalUtils import calc_cm_metrics\n",
    "\n",
    "from models.SimpleAutoEncoder import SimpleAutoEncoder\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_labels = df_data_anormal['label']\n",
    "df_data_anormal.drop('label', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleAutoEncoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=17, out_features=12, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=12, out_features=8, bias=True)\n",
       "    (3): Tanh()\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=8, out_features=12, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=12, out_features=17, bias=True)\n",
       "    (3): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_inpus = len(df_data_anormal.columns)\n",
    "val_lambda = 42 * 0.01\n",
    "fn_now = os.path.join(model_path, 'model_bib' ,'20200303_firstAE_model.pt')\n",
    "model = SimpleAutoEncoder(num_inputs=num_inpus, val_lambda=val_lambda)\n",
    "model.load_state_dict(torch.load(fn_now))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of anormal data: (35040, 18)\n"
     ]
    }
   ],
   "source": [
    "data_fn_anormal = os.path.join(data_path, 'anomalous_data_y_2022_reduced.h5')\n",
    "df_data_anormal = pd.read_hdf(data_fn_anormal, key='df')\n",
    "print('Shape of anormal data: {}'.format(df_data_anormal.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_train = MinMaxScaler((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_train = scaler_train.fit(df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35040, 17)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data_anormal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_anormal = scaler_train.transform(df_data_anormal.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build tensor from numpy\n",
    "anormal_torch_tensor = torch.from_numpy(scaled_anormal).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build TensorDataset from Tensor\n",
    "anormal_dataset = TensorDataset(anormal_torch_tensor, anormal_torch_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build DataLoader from TensorDataset\n",
    "anormal_dataloader = torch.utils.data.DataLoader(anormal_dataset,batch_size=128,shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_anormal = []\n",
    "for val in anormal_torch_tensor:\n",
    "    loss = model.calc_reconstruction_error(val)\n",
    "    losses_anormal.append(loss.item())\n",
    "    \n",
    "s_losses_anormal = pd.Series(losses_anormal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = s_losses_anormal.to_numpy()\n",
    "X = X.reshape(-1, 1)\n",
    "y = [1 if x > 0 else 0 for x in s_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight={1: 2.0}, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=42, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=42, fit_intercept=True, solver='liblinear', class_weight={1:2.0})\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for val in X:\n",
    "    val = val.reshape(1,-1)\n",
    "    pred = clf.predict(val)\n",
    "    predictions.append(pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y, predictions)\n",
    "tn, fp, fn, tp  = confusion_matrix(y, predictions).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 99.70034246572497\n",
      "Precision: 99.99999999958195\n",
      "Specifity: 93.15299842562096\n",
      "Sensitivity: 95.79495394435004\n",
      "F1-Score: 97.85232153794672\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, specifity, sensitivity, f1_score = calc_cm_metrics(tp, tn, fp, fn)\n",
    "print('Accuracy: {}'.format(accuracy))\n",
    "print('Precision: {}'.format(precision))\n",
    "print('Specifity: {}'.format(specifity))\n",
    "print('Sensitivity: {}'.format(sensitivity))\n",
    "print('F1-Score: {}'.format(f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/torge/Development/master/masterthesis_code/02_Experimente/AnomalyDetection/models/model_bib/20200303_LogRegModell.save']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fn = '{}_LogRegModell.save'.format(arrow.now().format('YYYYMMDD'))\n",
    "filename = os.path.join(model_bib_path, model_fn)\n",
    "joblib.dump(clf, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ma_modells",
   "language": "python",
   "name": "ma_modells"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
